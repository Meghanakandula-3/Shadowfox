{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSJgZkUBUUFKOYrTuH5aQU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meghanakandula-3/Shadowfox/blob/main/LanguageModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8W5yZbcwzCaU"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "ko0IXR2OzR6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "JMnvTDXdzid2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "kWCqupM5zwLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1\n",
        "fill_mask(\"The capital of France is [MASK].\")\n",
        "\n",
        "# Example 2\n",
        "fill_mask(\"Machine learning is a [MASK] field of computer science.\")\n"
      ],
      "metadata": {
        "id": "ZvqOL41Fz2KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"The [MASK] barked loudly at the mailman.\")\n",
        "fill_mask(\"She felt [MASK] after her long run.\")\n"
      ],
      "metadata": {
        "id": "3t7qCkUQz5PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zE4WMd6Vz-sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How accurately does BERT predict masked tokens in varied domains (e.g., medical, legal)?\n",
        "2. Does BERT understand context better when more words are provided?\n",
        "3. How does BERT handle ambiguity in sentences with multiple meanings?\n",
        "4. What are BERT’s limitations in language generation compared to GPT-style models?\n"
      ],
      "metadata": {
        "id": "Nu18-lPn0GsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results = fill_mask(\"The capital of Germany is [MASK].\")\n",
        "df = pd.DataFrame(results)\n",
        "sns.barplot(x=\"score\", y=\"token_str\", data=df)\n",
        "plt.title(\"Top Token Predictions by BERT\")\n",
        "plt.xlabel(\"Confidence Score\")\n",
        "plt.ylabel(\"Predicted Token\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ozpVg0cW0IV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bmz0ndqc0LNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Does BERT reflect societal biases (e.g., gender, race)?\n",
        "- Are its predictions influenced by stereotypical associations?\n",
        "- How to mitigate such biases? (e.g., fine-tuning on diverse datasets)\n"
      ],
      "metadata": {
        "id": "TGiVt2H-0O0T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TSx3ow9i0PfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BERT performs well in understanding masked tokens.\n",
        "- It relies heavily on context for prediction.\n",
        "- Limited creativity — not suitable for open-ended generation like GPT.\n",
        "- Powerful for classification, QA, and entity recognition tasks.\n"
      ],
      "metadata": {
        "id": "t8U47G-m0U4u"
      }
    }
  ]
}